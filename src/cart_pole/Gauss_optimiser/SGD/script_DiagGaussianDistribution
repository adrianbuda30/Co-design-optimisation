import torch
import torch.distributions as D
import torch.nn.functional as F
import numpy as np
import torch.optim as optim

class DesignDistribution_log:
    def __init__(self, initial_mean, initial_std, lr_mean=0.01, lr_std=0.01, optimizer_type="adam"):
        self.mean = torch.tensor(initial_mean, dtype=torch.float32, requires_grad=True)
        self.std = torch.tensor(initial_std, dtype=torch.float32, requires_grad=True)
        
        if optimizer_type == "adam":
            self.optimizer_mean = optim.Adam([self.mean], lr=lr_mean)
            self.optimizer_std = optim.Adam([self.std], lr=lr_std)
        elif optimizer_type == "sgd":
            self.optimizer_mean = optim.SGD([self.mean], lr=lr_mean)
            self.optimizer_std = optim.SGD([self.std], lr=lr_std)
        else:
            raise ValueError(f"Unsupported optimizer_type: {optimizer_type}")

    def update_distribution(self, batch_rewards, batch_samples):
        mean_rewards = torch.mean(torch.tensor(batch_rewards, dtype=torch.float32))
        grad_mean = torch.zeros_like(self.mean)
        grad_std = torch.zeros_like(self.std)

        for i in range(len(batch_rewards)):
            self.optimizer_mean.zero_grad()
            self.optimizer_std.zero_grad()
            sample = torch.tensor(batch_samples[i], dtype=torch.float32)
            neg_log_likelihood = D.Normal(self.mean, F.softplus(self.std)).log_prob(sample).sum()
            neg_log_likelihood.backward(retain_graph=True)
            grad_mean += self.mean.grad
            grad_std += self.std.grad

        grad_mean /= len(batch_rewards)
        grad_std /= len(batch_rewards)
        self.mean.grad = grad_mean * mean_rewards
        self.std.grad = grad_std * mean_rewards
        self.optimizer_mean.step()
        self.optimizer_std.step()

    def sample_design(self):
        return D.Normal(self.mean, F.softplus(self.std)).sample()

def reward_function(sampled_design):
    reward = torch.norm((sampled_design - torch.tensor([10.0, 20.0, 30.0, 40.0], dtype=torch.float32)))
    return reward.item()

initial_mean = 25 * np.random.rand(4).astype(np.float32)
initial_std = np.ones(4, dtype=np.float32) * 100  # Initialize std deviation as you prefer

# Choose optimizer type here: "adam" or "sgd"
optimizer_choice = "sgd" # or "sgd"
design_dist = DesignDistribution_log(initial_mean, initial_std, optimizer_type=optimizer_choice)

num_episodes = 100000
batch_size = 1

for i in range(0, num_episodes, batch_size):
    batch_rewards = []
    batch_samples = []

    for _ in range(batch_size):
        sampled_design = design_dist.sample_design().detach()
        reward = reward_function(sampled_design)
        batch_samples.append(sampled_design.numpy())
        batch_rewards.append(reward)

    design_dist.update_distribution(batch_rewards, batch_samples)
    print(f"mean: {design_dist.mean} reward: {np.mean(batch_rewards)}")
